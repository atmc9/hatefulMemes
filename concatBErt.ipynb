{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "concatBErt.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1ce4ad80f08249298f8cb25271cc7993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5a33347e4b24418d992fba85313db161",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_241b2c3b46e84c3a9ac485f787df3245",
              "IPY_MODEL_db5836eb1c9646a3841aa91beb726dfe"
            ]
          }
        },
        "5a33347e4b24418d992fba85313db161": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "241b2c3b46e84c3a9ac485f787df3245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9fc5d5ad809545b39f6dbf6f60a74c21",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8e85a171e33543619fa27dadff640d7c"
          }
        },
        "db5836eb1c9646a3841aa91beb726dfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5ad20bcca4b048de81212ebf437c42f9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/133 [00:20&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cf656517d36f4985a9d6c3457d7bcddb"
          }
        },
        "9fc5d5ad809545b39f6dbf6f60a74c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8e85a171e33543619fa27dadff640d7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5ad20bcca4b048de81212ebf437c42f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cf656517d36f4985a9d6c3457d7bcddb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "enGjb2_Vh6Rw",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce4464bb-2457-4e0c-97d7-acf698595073"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQC_qegfh-7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b5631477-7dc2-4ba2-b58c-7270f96979f2"
      },
      "source": [
        "%cd drive/My\\ Drive/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1itEhNqiJSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install pretrainedmodels\n",
        "# !pip install pytorch_pretrained_bert pytorch-nlp"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43NdLfiriJYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from pytorch_pretrained_bert import BertModel,BertAdam,BertTokenizer\n",
        "import torchvision.models as M\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torchvision.transforms import transforms\n",
        "from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve\n",
        "import pandas as pd\n",
        "# from transformers import BertTokenizer\n",
        "import tqdm.notebook as tq\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import functools\n",
        "import json\n",
        "import pretrainedmodels.models\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSUuXlPWoHbL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocab(object):\n",
        "    def __init__(self, emptyInit=False):\n",
        "        if emptyInit:\n",
        "            self.stoi, self.itos, self.vocab_sz = {}, [], 0\n",
        "        else:\n",
        "            self.stoi = {\n",
        "                w: i\n",
        "                for i, w in enumerate([\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"])\n",
        "            }\n",
        "            self.itos = [w for w in self.stoi]\n",
        "            self.vocab_sz = len(self.itos)\n",
        "\n",
        "    def add(self, words):\n",
        "        cnt = len(self.itos)\n",
        "        for w in words:\n",
        "            if w in self.stoi:\n",
        "                continue\n",
        "            self.stoi[w] = cnt\n",
        "            self.itos.append(w)\n",
        "            cnt += 1\n",
        "        self.vocab_sz = len(self.itos)\n",
        "\n",
        "def truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\n",
        "    Copied from https://github.com/huggingface/pytorch-pretrained-BERT\n",
        "    \"\"\"\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "\n",
        "def numpy_seed(seed, *addl_seeds):\n",
        "    \"\"\"Context manager which seeds the NumPy PRNG with the specified seed and\n",
        "    restores the state afterward\"\"\"\n",
        "    if seed is None:\n",
        "        yield\n",
        "        return\n",
        "    if len(addl_seeds) > 0:\n",
        "        seed = int(hash((seed, *addl_seeds)) % 1e6)\n",
        "    state = np.random.get_state()\n",
        "    np.random.seed(seed)\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        np.random.set_state(state)\n",
        "\n",
        "\n",
        "def get_vocab(hparams):\n",
        "    vocab = Vocab()\n",
        "    model=hparams.get('model','concatbert')\n",
        "    if model in [\"bert\", \"mmbt\", \"concatbert\"]:\n",
        "        bert_tokenizer = BertTokenizer.from_pretrained(\n",
        "            'bert-base-uncased', do_lower_case=True\n",
        "        )\n",
        "        vocab.stoi = bert_tokenizer.vocab\n",
        "        vocab.itos = bert_tokenizer.ids_to_tokens\n",
        "        vocab.vocab_sz = len(vocab.itos)\n",
        "\n",
        "\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def trainTransforms():\n",
        "  return transforms.Compose(\n",
        "      [\n",
        "       transforms.Resize(256),\n",
        "       transforms.CenterCrop(224),\n",
        "       \n",
        "       transforms.RandomRotation(15),\n",
        "       transforms.RandomHorizontalFlip(),\n",
        "       transforms.ColorJitter(brightness=0.4,\n",
        "                               contrast=0.4,\n",
        "                               saturation=0.4,\n",
        "                               hue=0.2),\n",
        "       transforms.ToTensor(),\n",
        "       transforms.Normalize(\n",
        "           mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "       ),\n",
        "       \n",
        "      ]\n",
        "  )\n",
        "def valTransforms():\n",
        "    return transforms.Compose(\n",
        "        [\n",
        "            transforms.Resize(299),\n",
        "            transforms.CenterCrop(299),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(\n",
        "                mean=[0.46777044, 0.44531429, 0.40661017],\n",
        "                std=[0.12221994, 0.12145835, 0.14380469],\n",
        "            ),\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "def set_seed(seed):\n",
        "  random.seed(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rO3RcMliJVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class memesDataset(Dataset):\n",
        "  def __init__(self,data_path,img_dir, tokenizer, transforms, vocab, hparams,test=False):\n",
        "    self.data = [json.loads(l) for l in open(data_path)]\n",
        "    # self.data_dir = os.path.dirname(data_path)\n",
        "    self.img_dir=img_dir\n",
        "    self.tokenizer = tokenizer\n",
        "    self.hparams = hparams\n",
        "    self.vocab = vocab\n",
        "    self.n_classes = 2\n",
        "    self.model=self.hparams.get(\"model\",'concatbert')\n",
        "    drop_image_percent=self.hparams.get(\"drop_image_percent\",0.0)\n",
        "    max_seq_len=self.hparams.get(\"max_seq_len\",200)\n",
        "    num_image_embeds=self.hparams.get(\"num_image_embeds\",1)\n",
        "    self.text_start_token = [\"[CLS]\"] if self.model != \"mmbt\" else [\"[SEP]\"]\n",
        "    self.test=test\n",
        "\n",
        "    # with numpy_seed(0):\n",
        "    #     for row in self.data:\n",
        "    #         if np.random.random() < drop_img_percent:\n",
        "    #             row[\"img\"] = None\n",
        "\n",
        "    self.max_seq_len = max_seq_len\n",
        "    if self.model == \"mmbt\":\n",
        "        self.max_seq_len -= num_image_embeds\n",
        "\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "    # return 10\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    \n",
        "    sentence = (self.text_start_token+ self.tokenizer(self.data[index][\"text\"])[: (self.max_seq_len - 1)])\n",
        "    segment = torch.zeros(len(sentence))\n",
        "\n",
        "    sentence = torch.LongTensor(\n",
        "        [\n",
        "            self.vocab.stoi[w] if w in self.vocab.stoi else self.vocab.stoi[\"[UNK]\"]\n",
        "            for w in sentence\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    id=self.data[index]['id']\n",
        "    if self.test==False:\n",
        "      label = torch.LongTensor([self.data[index][\"label\"]])\n",
        "    else:\n",
        "      label=None\n",
        "\n",
        "    image = None\n",
        "    if self.model in [\"img\", \"concatbow\", \"concatbert\", \"mmbt\",\"ensemble\"]:\n",
        "      \n",
        "      \n",
        "      image = Image.open(os.path.join(self.img_dir, self.data[index][\"img\"])).convert(\"RGB\")\n",
        "      # print(type(image))\n",
        "      image = self.transforms(image)\n",
        "    \n",
        "    if self.model == \"mmbt\":\n",
        "      # The first SEP is part of Image Token.\n",
        "      segment = segment[1:]\n",
        "      sentence = sentence[1:]\n",
        "      # The first segment (0) is of images.\n",
        "      segment += 1\n",
        "\n",
        "    return sentence, segment, image, label\n",
        "   "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ka-GTRO1O-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def collate_fn(batch, hparams,test=False):\n",
        "    lens = [len(row[0]) for row in batch]\n",
        "    bsz, max_seq_len = len(batch), max(lens)\n",
        "\n",
        "    mask_tensor = torch.zeros(bsz, max_seq_len).long()\n",
        "    text_tensor = torch.zeros(bsz, max_seq_len).long()\n",
        "    segment_tensor = torch.zeros(bsz, max_seq_len).long()\n",
        "    model=hparams.get('model','concatbert')\n",
        "    img_tensor = None\n",
        "    \n",
        "    if model in [\"img\", \"concatbow\", \"concatbert\", \"mmbt\",\"ensemble\"]:\n",
        "        img_tensor = torch.stack([row[2] for row in batch])\n",
        "\n",
        "    if test==False:\n",
        "      \n",
        "      tgt_tensor = torch.cat([row[3] for row in batch]).long()\n",
        "    else:\n",
        "      tgt_tensor=None\n",
        "\n",
        "    for i_batch, (input_row, length) in enumerate(zip(batch, lens)):\n",
        "        tokens, segment = input_row[:2]\n",
        "        text_tensor[i_batch, :length] = tokens\n",
        "        segment_tensor[i_batch, :length] = segment\n",
        "        mask_tensor[i_batch, :length] = 1\n",
        "    # id=[row[4] for row in batch]\n",
        "\n",
        "    #uncomment for test set\n",
        "    # return_val={'txt':text_tensor,'segment':segment_tensor,'mask':mask_tensor,'img':img_tensor,'tgt':tgt_tensor,'id':id}\n",
        "    return text_tensor, segment_tensor, mask_tensor, img_tensor, tgt_tensor\n",
        "    # return return_val"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtmaU3jz13iA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vocab=None\n",
        "# vocab_sz=None\n",
        "\n",
        "def get_loaders(hparams):\n",
        "\n",
        "  tokenizer = (BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True).tokenize)\n",
        "  \n",
        "  # transforms=get_transforms()\n",
        "  global vocab\n",
        "  global vocab_sz\n",
        "  vocab = get_vocab(hparams)\n",
        "  vocab_sz = vocab.vocab_sz\n",
        "  n_classes=2\n",
        "  train_path=hparams.get(\"train_path\")\n",
        "  img_dir=hparams.get(\"img_dir\")\n",
        "  train_transform=trainTransforms()\n",
        "  val_transform=valTransforms()\n",
        "  train_set=memesDataset(train_path,img_dir,tokenizer,val_transform,vocab,hparams)\n",
        "  dev_path=hparams.get(\"dev_path\")\n",
        "  dev_set=memesDataset(dev_path,img_dir,tokenizer,val_transform,vocab,hparams)\n",
        "  test_path=hparams.get(\"test_path\")\n",
        "  test_set=memesDataset(test_path,img_dir,tokenizer,val_transform,vocab,hparams,test=True)\n",
        "\n",
        "\n",
        "  collate = functools.partial(collate_fn,hparams=hparams,test=False)\n",
        "\n",
        "\n",
        "\n",
        "  batch_size=hparams.get('batch_size',8)\n",
        "  num_workers=hparams.get('num_workers',20)\n",
        "  \n",
        "  train_dataloader=DataLoader(train_set,shuffle=True,batch_size=batch_size,num_workers=num_workers,collate_fn=collate)\n",
        "  dev_dataloader=DataLoader(dev_set,shuffle=False,batch_size=batch_size,num_workers=num_workers,collate_fn=collate)\n",
        "  collate = functools.partial(collate_fn,hparams=hparams,test=True)\n",
        "  test_dataloader=DataLoader(test_set,shuffle=False,batch_size=batch_size,num_workers=num_workers,collate_fn=collate)\n",
        "  # print(np.shape(train_dataloader))\n",
        "  # text_tensor, segment_tensor, mask_tensor, img_tensor, tgt_tensor,id=test_dataloader\n",
        "\n",
        "  # test_loader={'txt':text_tensor,'segment':segment_tensor,'mask':mask_tensor,'img':img_tensor,'id':id}\n",
        "\n",
        "  return train_dataloader,dev_dataloader,test_dataloader,len(train_set)\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsIeMsK5h_BE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x\n",
        "class imageEnc(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "        super(imageEnc, self).__init__()\n",
        "        self.hparams = hparams\n",
        "        self.model=self.hparams.get(\"vision_model\",resnet152)\n",
        "        # self.model = torchvision.models.resnet152(pretrained=True)\n",
        "        self.modules = list(self.model.children())[:-2] #2 for resnet\n",
        "        # # print(self.modules)\n",
        "        \n",
        "        self.model = nn.Sequential(*self.modules).to(device)\n",
        "        \n",
        "\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1)) if self.hparams.get(\"pool\") == \"avg\" else nn.AdaptiveMaxPool2d((1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # x=self.model(x)\n",
        "        # x=x.logits \n",
        "        #for resnet use this thing\n",
        "        # print(np.shape(x))\n",
        "        x = self.pool(self.model(x))  \n",
        "        # x=self.model(x)\n",
        "        \n",
        "        x = torch.flatten(x, start_dim=2) #2 for resnet\n",
        "        x = x.transpose(1, 2).contiguous()  ##2 for resnet\n",
        "        # print(np.shape(x))\n",
        "        return x\n",
        "\n",
        "class bertEnc(nn.Module):\n",
        "    def __init__(self,hparmas):\n",
        "        super(bertEnc,self).__init__()\n",
        "        self.hparams=hparmas\n",
        "        self.model=BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "    def forward(self,txt, mask, segment):\n",
        "        _, out = self.model(txt,token_type_ids=segment,attention_mask=mask,output_all_encoded_layers=False,)\n",
        "        return out\n",
        "# class BertEncoder(nn.Module):\n",
        "#     def __init__(self, hparams):\n",
        "#         super(BertEncoder, self).__init__()\n",
        "#         self.hparams = hparams\n",
        "#         self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "#     def forward(self, txt, mask, segment):\n",
        "#         _, out = self.bert(\n",
        "#             txt,\n",
        "#             token_type_ids=segment,\n",
        "#             attention_mask=mask,\n",
        "#             output_all_encoded_layers=False,\n",
        "#         )\n",
        "#         return out\n",
        "\n",
        "\n",
        "class BertClf(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "        super(BertClf, self).__init__()\n",
        "        self.hparams = hparams\n",
        "        self.enc = BertEncoder(hparams)\n",
        "        self.clf = nn.Linear(768,2)\n",
        "        self.clf.apply(self.enc.bert.init_bert_weights)\n",
        "\n",
        "    def forward(self, txt, mask, segment):\n",
        "        x = self.enc(txt, mask, segment)\n",
        "        return self.clf(x)\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9US9XVXYkBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class imageBert(nn.Module):\n",
        "  def __init__(self,hparams,embeddings):\n",
        "    super(imageBert,self).__init__()\n",
        "    self.hparams=hparams\n",
        "    self.image_hidden_size=self.hparams.get(\"image_hidden_size\",2048)\n",
        "    self.hidden_size=self.hparams.get('hidden_sz',768)\n",
        "    self.img_embeddings=nn.Linear(self.image_hidden_size,self.hidden_size).to(device)\n",
        "    self.position_embeddings=embeddings.position_embeddings\n",
        "    self.token_type_embeddings=embeddings.token_type_embeddings\n",
        "    self.word_embeddings=embeddings.word_embeddings\n",
        "    self.LayerNorm = embeddings.LayerNorm\n",
        "    self.dropout = nn.Dropout(p=0.1)\n",
        "\n",
        "  def forward(self,input_imgs,token_type_ids):\n",
        "    bsz=input_imgs.size(0)\n",
        "    num_image_embeds=self.hparams.get(\"num_image_embeds\",1)\n",
        "    seq_length=num_image_embeds+2\n",
        "    cls_id = torch.LongTensor([vocab.stoi[\"[CLS]\"]]).cuda()\n",
        "    cls_id = cls_id.unsqueeze(0).expand(bsz, 1)\n",
        "    cls_token_embeds = self.word_embeddings(cls_id)\n",
        "\n",
        "    sep_id = torch.LongTensor([vocab.stoi[\"[SEP]\"]]).cuda()\n",
        "    sep_id = sep_id.unsqueeze(0).expand(bsz, 1)\n",
        "    sep_token_embeds = self.word_embeddings(sep_id)\n",
        "\n",
        "    imgs_embeddings = self.img_embeddings(input_imgs)\n",
        "    # print(np.shape(imgs_embeddings))\n",
        "    token_embeddings = torch.cat(\n",
        "        [cls_token_embeds, imgs_embeddings, sep_token_embeds], dim=1\n",
        "    )\n",
        "\n",
        "    position_ids = torch.arange(seq_length, dtype=torch.long).cuda()\n",
        "    position_ids = position_ids.unsqueeze(0).expand(bsz, seq_length)\n",
        "    position_embeddings = self.position_embeddings(position_ids)\n",
        "    token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "   \n",
        "    embeddings = token_embeddings + position_embeddings + token_type_embeddings\n",
        "    embeddings = self.LayerNorm(embeddings)\n",
        "    embeddings = self.dropout(embeddings)\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "\n",
        "class MultimodalBertEncoder(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "      super(MultimodalBertEncoder, self).__init__()\n",
        "      self.hparams = hparams\n",
        "      bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "      self.txt_embeddings = bert.embeddings\n",
        "\n",
        "      self.img_embeddings = imageBert(self.hparams, self.txt_embeddings).to(device)\n",
        "      self.img_encoder = imageEnc(self.hparams).to(device)\n",
        "      self.encoder = bert.encoder\n",
        "      self.pooler = bert.pooler\n",
        "      self.hidden_sz=self.hparams.get(\"hidden_sz\",768)\n",
        "      self.n_classes=self.hparams.get('n_classes',2)\n",
        "      self.clf = nn.Linear(self.hidden_sz, self.n_classes).to(device)\n",
        "      self.num_image_embeds=self.hparams.get(\"num_image_embeds\",1)\n",
        "\n",
        "\n",
        "    def forward(self, input_txt, attention_mask, segment, input_img):\n",
        "      bsz = input_txt.size(0)\n",
        "      attention_mask = torch.cat(\n",
        "          [\n",
        "              torch.ones(bsz, self.num_image_embeds + 2).long().cuda(),\n",
        "              attention_mask,\n",
        "          ],\n",
        "          dim=1,\n",
        "      )\n",
        "      extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "      extended_attention_mask = extended_attention_mask.to(\n",
        "          dtype=next(self.parameters()).dtype\n",
        "      )\n",
        "      extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "      img_tok = (\n",
        "          torch.LongTensor(input_txt.size(0), self.num_image_embeds + 2)\n",
        "          .fill_(0)\n",
        "          .cuda()\n",
        "      )\n",
        "      img = self.img_encoder(input_img)  # BxNx3x224x224 -> BxNx2048\n",
        "      img_embed_out = self.img_embeddings(img, img_tok)\n",
        "      txt_embed_out = self.txt_embeddings(input_txt, segment)\n",
        "      encoder_input = torch.cat([img_embed_out, txt_embed_out], 1)  # Bx(TEXT+IMG)xHID\n",
        "      \n",
        "      \n",
        "      encoded_layers = self.encoder(\n",
        "          encoder_input, extended_attention_mask, output_all_encoded_layers=False\n",
        "      )\n",
        "\n",
        "      return self.pooler(encoded_layers[-1])\n",
        "\n",
        "\n",
        "class MultimodalBertClf(nn.Module):\n",
        "    def __init__(self, hparams):\n",
        "        super(MultimodalBertClf, self).__init__()\n",
        "        self.hparams = hparams\n",
        "        self.enc = MultimodalBertEncoder(self.hparams)\n",
        "        hidden_sz=self.hparams.get('hidden_sz',768)\n",
        "        n_classes=2\n",
        "        self.clf = nn.Linear(hidden_sz, n_classes).to(device)\n",
        "\n",
        "    def forward(self, txt, mask, segment, img):\n",
        "        x = self.enc(txt, mask, segment, img)\n",
        "        return self.clf(x)\n",
        "\n",
        "class ConcatBert(nn.Module):\n",
        "  def __init__(self,hparams):\n",
        "    super(ConcatBert,self).__init__()\n",
        "    self.hparams=hparams\n",
        "    self.image=imageEnc(self.hparams)\n",
        "    self.bert=bertEnc(self.hparams)\n",
        "    self.last_size=768+2048\n",
        "    self.clf=nn.Linear(self.last_size,2).to(device)\n",
        "    self.hiddens=[1000,500,300]\n",
        "    self.main_layers=nn.ModuleList()\n",
        "    # for size in self.hiddens:\n",
        "    #   self.main_layers.append(nn.Linear(self.last_size,size))\n",
        "    #   self.main_layers.append(nn.BatchNorm1d(size))\n",
        "    #   self.main_layers.append(nn.Dropout(0.1))\n",
        "    #   self.main_layers.append(nn.ReLU())\n",
        "    #   self.last_size=size\n",
        "    self.main_layers.append(nn.Linear(self.last_size,768).to(device))\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,txt, mask, segment,image):\n",
        "    text=self.bert(txt, mask, segment)\n",
        "    image=self.image(image)\n",
        "    image=torch.flatten(image,start_dim=1)\n",
        "    \n",
        "    out=torch.cat([text,image],-1)\n",
        "    \n",
        "    for layer in self.main_layers:\n",
        "      out=layer(out)\n",
        "\n",
        "    return out\n",
        "    \n",
        "\n",
        "class ensembleBert(nn.Module):\n",
        "  def __init__(self,hparams):\n",
        "    super(ensembleBert,self).__init__()\n",
        "    self.hparams=hparams\n",
        "    self.concatBert=ConcatBert(self.hparams)\n",
        "    self.mmbt=MultimodalBertClf(self.hparams)\n",
        "    self.enc = MultimodalBertEncoder(self.hparams)\n",
        "    hidden_sz=self.hparams.get('hidden_sz',768)\n",
        "    n_classes=2\n",
        "    self.clf = nn.Linear(hidden_sz, n_classes).to(device)\n",
        "\n",
        "    \n",
        "  def forward(self,txt,mask,segment,image):\n",
        "    x1 = self.enc(txt, mask, segment, image)\n",
        "  \n",
        "    x2=self.concatBert(txt,mask,segment,image)\n",
        "    \n",
        "    x=torch.cat([x1,x2],1)\n",
        "    \n",
        "    x=nn.Linear(2*768,768).to(device)(x)\n",
        "    x=nn.Linear(768,2).to(device)(x)\n",
        "  \n",
        "    \n",
        "    return x\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnU2j_jMdybm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_optimizer(model,hparams,length):\n",
        "  # total_steps=length\n",
        "  batch_size=hparams.get(\"batch_size\",128)\n",
        "  gradient_accumulation_steps=hparams.get(\"gradient_accumulation_steps\",1)\n",
        "  max_epochs=hparams.get(\"max_epochs\",40)\n",
        "  lr=hparams.get(\"lr\",0.0001)\n",
        "  warmup=hparams.get('warmup',0.1)\n",
        "  total_steps = ((length/ batch_size/ gradient_accumulation_steps)* max_epochs)\n",
        "  # int( math.ceil(len(train_examples) / args.train_batch_size) / args.gradient_accumulation_steps) * args.num_train_epochs \n",
        "  param_optimizer = list(model.named_parameters())\n",
        "  no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
        "  optimizer_grouped_parameters = [\n",
        "      {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], \"weight_decay\": 0.01},\n",
        "      {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0,},\n",
        "  ]\n",
        "  optimizer = BertAdam(\n",
        "      optimizer_grouped_parameters,\n",
        "      lr=lr,\n",
        "      warmup=warmup,\n",
        "      t_total=total_steps,\n",
        "  )\n",
        "  return optimizer\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFiP4I3Cdyej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,criterion,samples,hparams):\n",
        "    model.train()\n",
        "    freeze_img=hparams.get(\"freeze_img\",0)\n",
        "    freeze_txt=hparams.get(\"freeze_text\",0)\n",
        "    \n",
        "    txt, segment, mask, img, label = samples\n",
        "    # print(img)\n",
        "    modelIdentifier=hparams.get('model',\"concatbert\")\n",
        "\n",
        "    if modelIdentifier==\"concatbert\":\n",
        "    \n",
        "      txt,segment,mask,img=txt.cuda(),segment.cuda(),mask.cuda(),img.cuda()\n",
        "      out=model(txt, mask, segment,img)\n",
        "\n",
        "    elif modelIdentifier==\"mmbt\":\n",
        "      # model=MultimodalBertClf(hparams)\n",
        "      for param in model.enc.img_encoder.parameters():\n",
        "        param.requires_grad = not freeze_img\n",
        "      for param in model.enc.encoder.parameters():\n",
        "        param.requires_grad = not freeze_txt\n",
        "      txt=txt.cuda()\n",
        "      img=img.cuda()\n",
        "      mask,segment=mask.cuda(),segment.cuda()\n",
        "      out=model(txt, mask, segment,img)\n",
        "    elif modelIdentifier == \"bert\":\n",
        "      txt, mask, segment = txt.cuda(), mask.cuda(), segment.cuda()\n",
        "      out = model(txt, mask, segment) \n",
        "    elif modelIdentifier==\"ensemble\":\n",
        "      for param in model.enc.img_encoder.parameters():\n",
        "        param.requires_grad = not freeze_img\n",
        "      for param in model.enc.encoder.parameters():\n",
        "        param.requires_grad = not freeze_txt\n",
        "      txt=txt.cuda()\n",
        "      img=img.cuda()\n",
        "      mask,segment=mask.cuda(),segment.cuda()\n",
        "      out=model(txt,mask,segment,img)\n",
        "      \n",
        "    # if test==False:\n",
        "    # if 'label' in \n",
        "    label=label.cuda()\n",
        "    loss=criterion(out,label)\n",
        "    # else:\n",
        "    #   loss=0\n",
        "    \n",
        "    #uncomment for test set\n",
        "    # return out,loss,label,id\n",
        "    return out,loss,label\n",
        "def eval(model,criterion,val_loader,hparams):\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    losses, preds, labels,outs = [], [], [],[]\n",
        "    for i,samples in enumerate(val_loader):\n",
        "      \n",
        "      out,loss,label=train(model,criterion,samples,hparams)\n",
        "      losses.append(loss.item())\n",
        "\n",
        "      \n",
        "            \n",
        "      pred = torch.nn.functional.softmax(out, dim=1).argmax(dim=1).cpu().detach().numpy()\n",
        "      out=out.cpu()\n",
        "      \n",
        "      outs+=out[:,1]\n",
        "      preds.append(pred)\n",
        "      label = label.cpu().detach().numpy()\n",
        "      labels.append(label)\n",
        "  metrics = {\"loss\": np.mean(losses)}\n",
        "  labels = [l for sl in labels for l in sl]\n",
        "  preds = [l for sl in preds for l in sl]\n",
        "  \n",
        "  outs=[l.item() for l in outs]\n",
        "  \n",
        "  \n",
        "  \n",
        "  metrics[\"acc\"] = accuracy_score(labels, preds)\n",
        "  metrics['auc']=roc_auc_score(labels,outs)\n",
        "  return metrics\n",
        "\n",
        "\n",
        "def testPred(model,criterion,test_set,test_loader,hparams):\n",
        "  model.eval()\n",
        "\n",
        "  id=test_set[4]\n",
        "  submission_frame = pd.DataFrame(columns=['id',\"proba\", \"label\"])\n",
        "  # print(len(id))\n",
        "  ids=[]\n",
        "  pred=[]\n",
        "  probs=[]\n",
        "  with torch.no_grad():\n",
        "    for batch in (tq.tqdm(test_loader)):\n",
        "      \n",
        "      \n",
        "      preds,_,_,id=train(model,criterion,batch,hparams,test=True)\n",
        "      preds=preds.cpu()\n",
        "      \n",
        "      ids+=id\n",
        "      pred+=(preds.argmax(dim=1))\n",
        "      probs+=(preds[:,1])\n",
        "      \n",
        "    \n",
        "    submission_frame['id']=ids\n",
        "    submission_frame['proba']=probs\n",
        "    submission_frame['label']=pred\n",
        "    \n",
        "    submission_frame.proba = submission_frame.proba.astype(float)\n",
        "    \n",
        "    submission_frame.label = submission_frame.label.astype(int)\n",
        "    return submission_frame\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoInWrvL7w3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def training(hparams):\n",
        "  criterion=nn.CrossEntropyLoss()\n",
        "  modelIdentifier=hparams.get(\"model\",\"concatbert\")\n",
        "  if modelIdentifier=='concatbert':\n",
        "    model=ConcatBert(hparams=hparams)\n",
        "  elif modelIdentifier=='mmbt':\n",
        "    model=MultimodalBertClf(hparams)\n",
        "  elif modelIdentifier=='bert':\n",
        "    model=BertClf(hparams)\n",
        "  elif modelIdentifier==\"ensemble\":\n",
        "    model=ensembleBert(hparams)\n",
        "  train_dataloader,dev_dataloader,test_dataloader,length=get_loaders(hparams)\n",
        "  optimizer=get_optimizer(model,hparams,length)\n",
        "  model.cuda()\n",
        "  batch_size=hparams.get(\"batch_size\",128)\n",
        "  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=5, factor=0.1, verbose=True)\n",
        "  gradient_accumulation_steps=hparams.get(\"gradient_accumulation_steps\",24)\n",
        "  start_epoch, global_step, n_no_improve, best_metric = 0, 0, 0, -np.inf\n",
        "  epochs=hparams.get(\"max_epochs\",40)\n",
        "  for i in range(1,epochs+1):\n",
        "    train_losses = []\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    accumulation_steps=hparams.get('gradient_accumulation_steps',256)\n",
        "    for samples in tq.tqdm(train_dataloader):\n",
        "      _,loss,_=train(model,criterion,samples,hparams)\n",
        "      if accumulation_steps>1:\n",
        "        loss=loss/accumulation_steps\n",
        "      loss.backward()\n",
        "      train_losses.append(loss.item())\n",
        "      global_step+=1\n",
        "      if global_step%accumulation_steps==0:\n",
        "        print(global_step)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    model.eval()\n",
        "    metrics=eval(model,criterion,dev_dataloader,hparams)\n",
        "    tuning_metric=metrics['auc']\n",
        "    scheduler.step(tuning_metric)\n",
        "    n_no_improve+=1\n",
        "    if tuning_metric>best_metric:\n",
        "      n_no_improve=0\n",
        "      if hparams.get(\"save_model\")=='yes':\n",
        "        save_name=hparams.get(\"save_name\")\n",
        "        torch.save(model,save_name)\n",
        "        print(\"model saved at epoch\",i)\n",
        "        best_metric=tuning_metric\n",
        "    if n_no_improve>=4:\n",
        "      print(\"early stopping triggered\")\n",
        "      break\n",
        "\n",
        "    print(\"epoch | {:2d} | train loss {:5.4f}\".format(i,np.mean(train_losses)) )\n",
        "    print(\"epoch | {} | val loss {} | val accuracy {} | auc {} |  \".format(i,metrics['loss'],metrics['acc'],metrics['auc']))\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0vbuxS3idAr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372,
          "referenced_widgets": [
            "1ce4ad80f08249298f8cb25271cc7993",
            "5a33347e4b24418d992fba85313db161",
            "241b2c3b46e84c3a9ac485f787df3245",
            "db5836eb1c9646a3841aa91beb726dfe",
            "9fc5d5ad809545b39f6dbf6f60a74c21",
            "8e85a171e33543619fa27dadff640d7c",
            "5ad20bcca4b048de81212ebf437c42f9",
            "cf656517d36f4985a9d6c3457d7bcddb"
          ]
        },
        "outputId": "17e8a424-2fe5-40a6-b30d-8864c94c14fb"
      },
      "source": [
        "\n",
        "train_path = \"data/train.jsonl\"\n",
        "dev_path = \"data/dev.jsonl\"\n",
        "test_path=\"data/test.jsonl\"\n",
        "set_seed(40)\n",
        "img_dir =os.path.join(os.getcwd(),'data')\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "resnet152=M.resnet152(pretrained=True)\n",
        "\n",
        "\n",
        "hparams = {\n",
        "\n",
        "    # Required hparams\n",
        "    \"train_path\": train_path,\n",
        "    \"dev_path\": dev_path,\n",
        "    \"img_dir\": img_dir,\n",
        "    \"test_path\":test_path,\n",
        "\n",
        "    # Optional hparams\n",
        "    \"embeddings\": 300,\n",
        "    \"language_feature_dim\": 300,\n",
        "    \"vision_feature_dim\": 300,\n",
        "    \"fusion_output_size\": 256,\n",
        "    \"output_path\": \"model-outputs\",\n",
        "    \"dev_limit\": None,\n",
        "    \"lr\": 0.0001,\n",
        "    \"max_epochs\": 10,\n",
        "    \"n_gpu\": 1,\n",
        "    \"batch_size\": 64,\n",
        "    # allows us to \"simulate\" having larger batches\n",
        "    \"accumulate_grad_batches\": 256,\n",
        "    # \"early_stop_patience\": 6,\n",
        "    \"device\":device,\n",
        "    \"pool\":\"max\",\n",
        "    \"save_model\":\"yes\",\n",
        "    \"model\":\"mmbt\",\n",
        "    \n",
        "    \"hidden_sz\":768,\n",
        "    \"vision_model\":resnet152,\n",
        "    \"save_name\":\"mmbt_ensemble.pt\",\n",
        "    \"image_hidden_size\":resnet152.fc.in_features\n",
        "    \n",
        "}\n",
        "if __name__==\"__main__\":\n",
        "  \n",
        "  training(hparams)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ce4ad80f08249298f8cb25271cc7993",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=133.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-b44120694817>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-c8577599e5bc>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(hparams)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gradient_accumulation_steps'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m       \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0maccumulation_steps\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0maccumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-10654d327d56>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, criterion, samples, hparams)\u001b[0m\n\u001b[1;32m     22\u001b[0m       \u001b[0mimg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m       \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m       \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodelIdentifier\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"bert\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m       \u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-f460c7b8773e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, txt, mask, segment, img)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-f460c7b8773e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_txt, attention_mask, segment, input_img)\u001b[0m\n\u001b[1;32m     79\u001b[0m           \u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m       )\n\u001b[0;32m---> 81\u001b[0;31m       \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# BxNx3x224x224 -> BxNx2048\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m       \u001b[0mimg_embed_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_tok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0mtxt_embed_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtxt_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_txt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-fbe307279865>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m#for resnet use this thing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# print(np.shape(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;31m# x=self.model(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 416\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 15.75 GiB total capacity; 14.31 GiB already allocated; 68.88 MiB free; 14.52 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2wloqcDwRp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWJ6RanF4VhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FP8Vjn3O4Zaf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}